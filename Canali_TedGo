###### TEDx-Load-Aggregate-Model
######

import sys
import json
import pyspark
from pyspark.sql.functions import col, collect_list, explode, flatten, lit, array_distinct

from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job



##### FROM FILES
tedx_dataset_path = "s3://tedx-2025-data-br-20252315/final_list.csv"

###### READ PARAMETERS
args = getResolvedOptions(sys.argv, ['JOB_NAME'])

##### START JOB CONTEXT AND JOB
sc = SparkContext()


glueContext = GlueContext(sc)
spark = glueContext.spark_session

    
job = Job(glueContext)
job.init(args['JOB_NAME'], args)



#### READ INPUT FILES TO CREATE AN INPUT DATASET
tedx_dataset = spark.read \
    .option("header","true") \
    .option("quote", "\"") \
    .option("escape", "\"") \
    .csv(tedx_dataset_path)
    
tedx_dataset.printSchema()


#### FILTER ITEMS WITH NULL POSTING KEY
count_items = tedx_dataset.count()
count_items_null = tedx_dataset.filter("id is not null").count()

print(f"Number of items from RAW DATA {count_items}")
print(f"Number of items from RAW DATA with NOT NULL KEY {count_items_null}")

## READ THE DETAILS
details_dataset_path = "s3://tedx-2025-data-br-20252315/details.csv"
details_dataset = spark.read \
    .option("header","true") \
    .option("quote", "\"") \
    .option("escape", "\"") \
    .csv(details_dataset_path)

details_dataset = details_dataset.select(col("id").alias("id_ref"), #trasformazione
                                         col("description"),
                                         col("duration"),
                                         col("publishedAt"))

# AND JOIN WITH THE MAIN TABLE (tra i due dataset faccio una left join, ovvero che potrei non avere dettagli nelle tabelle)
tedx_dataset_main = tedx_dataset.join(details_dataset, tedx_dataset.id == details_dataset.id_ref, "left") \
    .drop("id_ref") #tolgo la tabella id_ref

tedx_dataset_main.printSchema()

## READ TAGS DATASET, prendo il file dei tag
tags_dataset_path = "s3://tedx-2025-data-br-20252315/tags.csv"
tags_dataset = spark.read.option("header","true").csv(tags_dataset_path) #leggo il file, senza impostare l'escape, leggo solo il csv


# CREATE THE AGGREGATE MODEL, ADD TAGS TO TEDX_DATASET
tags_dataset_agg = tags_dataset.groupBy(col("id").alias("id_ref")).agg(collect_list("tag").alias("tags")) #faccio una groupby id, 
#per aggregare i talk con .agg e l'api collectlist per avere una colonna di array di tag
tags_dataset_agg.printSchema() 
#join tra il dataset di prima, con la condizione di join per id sempre left
tedx_dataset_agg = tedx_dataset_main.join(tags_dataset_agg, tedx_dataset.id == tags_dataset_agg.id_ref, "left") \
    .drop("id_ref") \
    .select(col("id").alias("_id"), col("*")) \
    .drop("id") \

tedx_dataset_agg.printSchema()

#ELIMINA TUTTE LE TUPLE CHE NON HANNO L'URL, IL TITOLO E LA DESCRIZIONE
tedx_dataset_agg=tedx_dataset_agg.filter((col("url").rlike(r"^https?://")) | (col("title").isNotNull()) | (col("description").isNotNull()))

#CANCELLA LE COLONNE CHE NON SONO DI NOSTRO INTERESSE
tedx_dataset_agg=tedx_dataset_agg.drop("slug").drop("speakers").drop("title").drop("url").drop("description").drop("publishedAt")

#CREO LA TABELLA CON SOLO ID, DURATA E TAG
tedx_dataset_explode=tedx_dataset_agg.select(col("_id"), col("duration"), explode(col("tags")).alias("tag"))

#ELIMINO LE TUPLE DUPLICATE
tedx_dataset_explode= tedx_dataset_explode.dropDuplicates()

#ELIMINIAMO LE TUPLE CHE HANNO DURATA NULLA E MAGGIORE DI 900 SECONDI
tedx_dataset_explode=tedx_dataset_explode.filter((col("duration") < 900) & (col("duration").isNotNull()))

#RAGGRUPPIAMO PER TAG TUTTI GLI ID ED ELIMINIAMO LA COLONNA DURATION
tedx_dataset_canali = tedx_dataset_explode.groupBy("tag") \
    .agg(collect_list("_id").alias("id_associati")) \
    .withColumnRenamed("tag", "_id").drop(col("duration"))

tedx_dataset_canali = tedx_dataset_canali.filter(
    (col("_id")=="science") |
    (col("_id")=="art") | (col("_id")=="design") |
    (col("_id")=="sports") | (col("_id")=="health") |
    (col("_id")=="politics") |
    (col("_id")=="technology") |
    (col("_id")=="economics") | (col("_id")=="business") |
    (col("_id")=="entertainment") |
    (col("_id")=="education") )

arte_design = tedx_dataset_canali.filter(col("_id").isin("arte", "design"))
sports_health = tedx_dataset_canali.filter(col("_id").isin("sports", "health"))
economics_business = tedx_dataset_canali.filter(col("_id").isin("economics", "business"))

a_d = arte_design.agg(array_distinct(flatten(collect_list("id_associati"))).alias("id_associati")) \
    .withColumn("_id", lit("arte_design"))
s_h = sports_health.agg(array_distinct(flatten(collect_list("id_associati"))).alias("id_associati")) \
    .withColumn("_id", lit("sports_health"))
e_b = economics_business.agg(array_distinct(flatten(collect_list("id_associati"))).alias("id_associati")) \
    .withColumn("_id", lit("economics_business"))

altri_id = tedx_dataset_canali.filter(col("_id").isin("science", "politics", "technology", "entertainment", "education"))

risultato_finale = altri_id.unionByName(a_d.select("_id", "id_associati"))
risultato_finale = risultato_finale.unionByName(s_h.select("_id", "id_associati"))
risultato_finale = risultato_finale.unionByName(e_b.select("_id", "id_associati"))


write_mongo_options = { #imposto i parametri di connessione con il mio db
    "connectionName": "TEDx",
    "database": "unibg_tedx_2025",
    "collection": "tedx_canale",
    "ssl": "true",
    "ssl.domain_match": "false"
}
from awsglue.dynamicframe import DynamicFrame
tedx_dataset_dynamic_frame = DynamicFrame.fromDF(risultato_finale, glueContext, "nested")


glueContext.write_dynamic_frame.from_options(tedx_dataset_dynamic_frame, connection_type="mongodb", connection_options=write_mongo_options)
